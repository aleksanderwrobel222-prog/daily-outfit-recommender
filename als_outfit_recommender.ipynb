{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Running ALS on Outfit Recommendation (PySpark)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "System version: 3.9.23 (main, Jun  5 2025, 13:40:20) \n",
                        "[GCC 11.2.0]\n",
                        "Spark version: 4.0.0\n"
                    ]
                }
            ],
            "source": [
                "import warnings\n",
                "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
                "\n",
                "import sys\n",
                "import pyspark\n",
                "from pyspark.ml.recommendation import ALS\n",
                "from pyspark.ml.feature import StringIndexer\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import StructType, StructField\n",
                "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
                "\n",
                "from recommenders.utils.timer import Timer\n",
                "from recommenders.utils.notebook_utils import is_jupyter\n",
                "from recommenders.datasets.spark_splitters import spark_random_split\n",
                "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
                "from recommenders.utils.spark_utils import start_or_get_spark\n",
                "from recommenders.utils.notebook_utils import store_metadata\n",
                "\n",
                "# Dataset\n",
                "from datasets import outfits\n",
                "\n",
                "print(f\"System version: {sys.version}\")\n",
                "print(\"Spark version: {}\".format(pyspark.__version__))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# top k items to recommend\n",
                "TOP_K = 1\n",
                "\n",
                "OUTFITS_DATA_SIZE = '100'\n",
                "\n",
                "# Column names for the dataset\n",
                "COL_USER = \"UserId\"\n",
                "COL_ITEM = \"Clothing\"\n",
                "COL_RATING = \"Rating\"\n",
                "COL_WEATHER = \"Weather\"\n",
                "COL_ITEM_ID = \"ClothingId\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start Spark session\n",
                "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\")\n",
                "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Download Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+-------+--------------------+------+\n",
                        "|UserId|Weather|            Clothing|Rating|\n",
                        "+------+-------+--------------------+------+\n",
                        "|    17|   Mild|    Crewneck Sweater|   4.2|\n",
                        "|     1|   Cool|              Chinos|   4.1|\n",
                        "|     2|   Cool|    Pleated Trousers|   3.1|\n",
                        "|    24|   Cold|    Crewneck Sweater|   3.5|\n",
                        "|    20|    Hot|    Pleated Trousers|   2.5|\n",
                        "|    18|  Rainy|    Pleated Trousers|   2.8|\n",
                        "|    25|   Cold|Oxford Cloth Butt...|   2.6|\n",
                        "|     1|   Cold|              Chinos|   2.9|\n",
                        "|    12|   Cold|      Linen Trousers|   1.0|\n",
                        "|     9|  Rainy|Oxford Cloth Butt...|   3.3|\n",
                        "|     3|   Cold|       Wool Trousers|   5.0|\n",
                        "|     2|  Sunny|         Linen Shirt|   4.1|\n",
                        "|    19|   Cool|    Pleated Trousers|   3.0|\n",
                        "|    15|   Cool|Oxford Cloth Butt...|   3.6|\n",
                        "|     7|  Rainy|       Wool Trousers|   3.6|\n",
                        "|    23|   Cool|    Pleated Trousers|   3.2|\n",
                        "|    14|    Hot|      V-Neck Sweater|   3.1|\n",
                        "|     2|  Sunny|       Wool Trousers|   1.2|\n",
                        "|    11|   Cool|      V-Neck Sweater|   3.9|\n",
                        "|    14|   Cold|    Crewneck Sweater|   3.2|\n",
                        "+------+-------+--------------------+------+\n",
                        "only showing top 20 rows\n",
                        "Data after indexing the 'Clothing' column:\n",
                        "+------+-------+--------------------+------+----------+\n",
                        "|UserId|Weather|            Clothing|Rating|ClothingId|\n",
                        "+------+-------+--------------------+------+----------+\n",
                        "|    17|   Mild|    Crewneck Sweater|   4.2|       5.0|\n",
                        "|     1|   Cool|              Chinos|   4.1|       1.0|\n",
                        "|     2|   Cool|    Pleated Trousers|   3.1|       0.0|\n",
                        "|    24|   Cold|    Crewneck Sweater|   3.5|       5.0|\n",
                        "|    20|    Hot|    Pleated Trousers|   2.5|       0.0|\n",
                        "|    18|  Rainy|    Pleated Trousers|   2.8|       0.0|\n",
                        "|    25|   Cold|Oxford Cloth Butt...|   2.6|       6.0|\n",
                        "|     1|   Cold|              Chinos|   2.9|       1.0|\n",
                        "|    12|   Cold|      Linen Trousers|   1.0|       7.0|\n",
                        "|     9|  Rainy|Oxford Cloth Butt...|   3.3|       6.0|\n",
                        "|     3|   Cold|       Wool Trousers|   5.0|       9.0|\n",
                        "|     2|  Sunny|         Linen Shirt|   4.1|       3.0|\n",
                        "|    19|   Cool|    Pleated Trousers|   3.0|       0.0|\n",
                        "|    15|   Cool|Oxford Cloth Butt...|   3.6|       6.0|\n",
                        "|     7|  Rainy|       Wool Trousers|   3.6|       9.0|\n",
                        "|    23|   Cool|    Pleated Trousers|   3.2|       0.0|\n",
                        "|    14|    Hot|      V-Neck Sweater|   3.1|       8.0|\n",
                        "|     2|  Sunny|       Wool Trousers|   1.2|       9.0|\n",
                        "|    11|   Cool|      V-Neck Sweater|   3.9|       8.0|\n",
                        "|    14|   Cold|    Crewneck Sweater|   3.2|       5.0|\n",
                        "+------+-------+--------------------+------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "schema = StructType(\n",
                "    (\n",
                "        StructField(COL_USER, IntegerType()),\n",
                "        StructField(COL_WEATHER, StringType()),\n",
                "        StructField(COL_ITEM, StringType()),\n",
                "        StructField(COL_RATING, FloatType()),\n",
                "    )\n",
                ")\n",
                "\n",
                "data = outfits.load_spark_df(spark, size=None, schema=schema, filepath=\"./datasets/csv/feature1.csv\")\n",
                "data.show()\n",
                "\n",
                "indexer = StringIndexer(inputCol=COL_ITEM, outputCol=COL_ITEM_ID)\n",
                "indexed_data = indexer.fit(data).transform(data)\n",
                "print(\"Data after indexing the 'Clothing' column:\")\n",
                "indexed_data.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Splitting the Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "N train 154\n",
                        "N test 46\n"
                    ]
                }
            ],
            "source": [
                "train, test = spark_random_split(indexed_data, ratio=0.75, seed=123)\n",
                "print (\"N train\", train.cache().count())\n",
                "print (\"N test\", test.cache().count())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Training the Model and Getting Our Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "header = {\n",
                "    \"userCol\": COL_USER,\n",
                "    \"itemCol\": COL_ITEM_ID,\n",
                "    \"ratingCol\": COL_RATING,\n",
                "}\n",
                "\n",
                "\n",
                "als = ALS(\n",
                "    rank=10,\n",
                "    maxIter=15,\n",
                "    implicitPrefs=False,\n",
                "    regParam=0.05,\n",
                "    coldStartStrategy='drop',\n",
                "    nonnegative=False,\n",
                "    seed=42,\n",
                "    **header\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 3.5284303050002563 seconds for training.\n"
                    ]
                }
            ],
            "source": [
                "with Timer() as train_time:\n",
                "    model = als.fit(train)\n",
                "\n",
                "print(\"Took {} seconds for training.\".format(train_time.interval))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 2134:=================================================>  (190 + 8) / 200]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 2.3500981329998467 seconds for prediction.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "# with Timer() as test_time:\n",
                "\n",
                "#     # Get the cross join of all user-item pairs and score them.\n",
                "#     users = train.select(COL_USER).distinct()\n",
                "#     items = train.select(COL_ITEM).distinct()\n",
                "#     user_item = users.crossJoin(items)\n",
                "#     dfs_pred = model.transform(user_item)\n",
                "\n",
                "#     # Remove seen items.\n",
                "#     dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
                "#         train.alias(\"train\"),\n",
                "#         (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
                "#         how='outer'\n",
                "#     )\n",
                "\n",
                "#     top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
                "#         .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
                "\n",
                "#     # In Spark, transformations are lazy evaluation\n",
                "#     # Use an action to force execute and measure the test time \n",
                "#     top_all.cache().count()\n",
                "\n",
                "# print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
                "\n",
                "with Timer() as test_time:\n",
                "    users = train.select(COL_USER).distinct()\n",
                "\n",
                "    items = train.select(COL_ITEM_ID).distinct()\n",
                "\n",
                "    user_item = users.crossJoin(items)\n",
                "    dfs_pred = model.transform(user_item)\n",
                "\n",
                "    top_all = dfs_pred.join(\n",
                "        indexed_data.select(COL_USER, COL_ITEM_ID),\n",
                "        on=[COL_USER, COL_ITEM_ID],\n",
                "        how='left_anti'\n",
                "    )\n",
                "\n",
                "    # Force execution to measure the time\n",
                "    top_all.cache().count()\n",
                "\n",
                "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+----------+----------+\n",
                        "|UserId|ClothingId|prediction|\n",
                        "+------+----------+----------+\n",
                        "|    22|         8| 2.4058814|\n",
                        "|     1|         8| 3.2283807|\n",
                        "|    13|         8| 2.8466918|\n",
                        "|     3|         8| 3.4562228|\n",
                        "|    20|         8| 3.2722473|\n",
                        "|    19|         8|  2.844372|\n",
                        "|    15|         8| 2.9901443|\n",
                        "|     9|         8| 3.4880786|\n",
                        "|     4|         8| 3.7692726|\n",
                        "|    25|         8| 3.1036925|\n",
                        "|     2|         8| 2.9269228|\n",
                        "|    18|         8|  3.415548|\n",
                        "|    16|         0| 2.7907774|\n",
                        "|    17|         0| 3.2832756|\n",
                        "|     4|         0| 3.5647602|\n",
                        "|     7|         0| 2.7452872|\n",
                        "|    25|         0|  2.718147|\n",
                        "|    21|         0| 2.8855975|\n",
                        "|    22|         7| 1.0010359|\n",
                        "|     1|         7| 1.9375099|\n",
                        "+------+----------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "top_all.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM_ID, \n",
                "                                    col_rating=COL_RATING, col_prediction=\"prediction\", \n",
                "                                    relevancy_method=\"top_k\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model:\tALS\n",
                        "Top K:\t1\n",
                        "MAP:\t0.000000\n",
                        "NDCG:\t0.000000\n",
                        "Precision@K:\t0.000000\n",
                        "Recall@K:\t0.000000\n"
                    ]
                }
            ],
            "source": [
                "print(\"Model:\\tALS\",\n",
                "      \"Top K:\\t%d\" % rank_eval.k,\n",
                "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
                "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
                "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
                "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluate Rating Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+-------+--------------------+------+----------+----------+\n",
                        "|UserId|Weather|            Clothing|Rating|ClothingId|prediction|\n",
                        "+------+-------+--------------------+------+----------+----------+\n",
                        "|    12|  Rainy|    Cashmere Sweater|   2.5|         2|  3.190533|\n",
                        "|    12|  Sunny|      Linen Trousers|   4.6|         7|  1.259581|\n",
                        "|    22|    Hot|       Wool Trousers|   1.0|         9| 2.8805444|\n",
                        "|    22|   Mild|              Chinos|   4.0|         1|  3.383572|\n",
                        "|     1|   Cold|          Polo Shirt|   1.0|         4| 3.4242387|\n",
                        "|     1|   Mild|              Chinos|   4.3|         1| 3.4575682|\n",
                        "|    13|   Cool|    Crewneck Sweater|   3.7|         5| 3.2912312|\n",
                        "|    13|  Rainy|              Chinos|   2.9|         1| 4.1585917|\n",
                        "|    16|   Cool|Oxford Cloth Butt...|   3.7|         6|  2.697968|\n",
                        "|    16|   Mild|       Wool Trousers|   2.9|         9| 2.6002858|\n",
                        "|     3|  Sunny|          Polo Shirt|   4.6|         4| 3.0810018|\n",
                        "|    20|   Mild|    Crewneck Sweater|   3.6|         5| 3.2652674|\n",
                        "|    20|   Mild|Oxford Cloth Butt...|   4.1|         6| 2.0806391|\n",
                        "|    20|  Rainy|              Chinos|   2.9|         1|  3.669273|\n",
                        "|    19|   Cool|    Cashmere Sweater|   3.3|         2| 3.3815417|\n",
                        "|    19|   Mild|      Linen Trousers|   2.5|         7| 1.1024182|\n",
                        "|    19|  Sunny|      Linen Trousers|   4.3|         7| 1.1024182|\n",
                        "|    15|   Cool|         Linen Shirt|   1.8|         3| 2.0337825|\n",
                        "|    15|    Hot|         Linen Shirt|   4.1|         3| 2.0337825|\n",
                        "|    15|    Hot|       Wool Trousers|   1.1|         9| 2.6894538|\n",
                        "+------+-------+--------------------+------+----------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "# Generate predicted ratings.\n",
                "prediction = model.transform(test)\n",
                "prediction.cache().show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model:\tALS rating prediction\n",
                        "RMSE:\t1.472280\n",
                        "MAE:\t1.176934\n",
                        "Explained variance:\t-0.819469\n",
                        "R squared:\t-1.130056\n"
                    ]
                }
            ],
            "source": [
                "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
                "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
                "\n",
                "print(\"Model:\\tALS rating prediction\",\n",
                "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
                "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
                "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
                "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+----------+---------+----------------+\n",
                        "|UserId|ClothingId|rating   |Clothing        |\n",
                        "+------+----------+---------+----------------+\n",
                        "|1     |5         |3.8331847|Crewneck Sweater|\n",
                        "|1     |2         |3.7567925|Cashmere Sweater|\n",
                        "|1     |1         |3.4575682|Chinos          |\n",
                        "|1     |4         |3.4242387|Polo Shirt      |\n",
                        "|1     |0         |3.230017 |Pleated Trousers|\n",
                        "+------+----------+---------+----------------+\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "from pyspark.ml.feature import IndexToString\n",
                "from pyspark.sql.functions import explode, col\n",
                "\n",
                "# Recommend items for all users\n",
                "user_recs = model.recommendForAllUsers(5)\n",
                "\n",
                "# explode recommendations into one row per (user, rec)\n",
                "recs_exploded = user_recs.select(COL_USER, explode(col(\"recommendations\")).alias(\"rec\"))\n",
                "\n",
                "# pull out the indexed item id (ClothingId) and rating into top-level columns\n",
                "recs_flat = recs_exploded.select(\n",
                "    COL_USER,\n",
                "    col(\"rec.\" + COL_ITEM_ID).alias(COL_ITEM_ID),\n",
                "    col(\"rec.rating\").alias(\"rating\"),\n",
                ")\n",
                "\n",
                "# Convert back to original item name\n",
                "inverter = IndexToString(inputCol=COL_ITEM_ID, outputCol=COL_ITEM, labels=indexer.fit(data).labels)\n",
                "itd = inverter.transform(recs_flat)\n",
                "\n",
                "# Show recommendations for a user\n",
                "itd.where(itd[COL_USER] == 1).show(truncate = 0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "# cleanup spark instance\n",
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "envLAI",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.23"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
